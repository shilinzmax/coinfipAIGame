# Google Colab + vLLM å®Œæ•´éƒ¨ç½²æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ åœ¨Google Colabä¸Šéƒ¨ç½²vLLMï¼Œå¹¶å°†å…¶é›†æˆåˆ°ä½ çš„æ·ç¡¬å¸æ¸¸æˆä¸­ï¼Œæä¾›çœŸå®çš„AIåˆ†æåŠŸèƒ½ã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

- Googleè´¦å·
- Colab Proè®¢é˜… ($10/æœˆ) - æ¨è
- ç¨³å®šçš„ç½‘ç»œè¿æ¥

## ğŸš€ éƒ¨ç½²æ­¥éª¤

### æ­¥éª¤1ï¼šæ‰“å¼€Google Colab

1. è®¿é—® [colab.research.google.com](https://colab.research.google.com)
2. ç™»å½•ä½ çš„Googleè´¦å·
3. ç‚¹å‡»"New notebook"
4. é‡å‘½åä¸º"vllm-coin-game"

### æ­¥éª¤2ï¼šå‡çº§åˆ°Colab Proï¼ˆæ¨èï¼‰

1. ç‚¹å‡»å³ä¸Šè§’"Upgrade"
2. é€‰æ‹©"Colab Pro" ($10/æœˆ)
3. å®Œæˆæ”¯ä»˜

**ä¸ºä»€ä¹ˆéœ€è¦Proï¼Ÿ**
- æ›´é•¿çš„GPUæ—¶é—´
- æ›´ç¨³å®šçš„è¿æ¥
- æ›´å¥½çš„æ€§èƒ½

### æ­¥éª¤3ï¼šåˆ›å»ºCell 1 - å®‰è£…ä¾èµ–å’Œä¸‹è½½æ¨¡å‹

ç‚¹å‡»"+"æŒ‰é’®ï¼Œé€‰æ‹©"Code"ï¼Œè¾“å…¥ä»¥ä¸‹ä»£ç ï¼š

```python
# Cell 1: å¹¶è¡Œå®‰è£…ä¾èµ–å’Œä¸‹è½½æ¨¡å‹
import subprocess
import threading
import time

def install_dependencies():
    print("ğŸ”§ å¼€å§‹å®‰è£…ä¾èµ–...")
    subprocess.run(['pip', 'install', 'vllm', 'transformers', 'torch', 'pyngrok'])
    print("âœ… ä¾èµ–å®‰è£…å®Œæˆï¼")

def download_model():
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹...")
    from transformers import AutoTokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')
        print("âœ… Llama-2æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ Llama-2ä¸‹è½½å¤±è´¥: {e}")
        print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹...")
        tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')
        print("âœ… å¤‡ç”¨æ¨¡å‹ä¸‹è½½å®Œæˆï¼")

def check_gpu():
    print("ğŸ–¥ï¸ æ£€æŸ¥GPUçŠ¶æ€...")
    subprocess.run(['nvidia-smi'])

# å¹¶è¡Œæ‰§è¡Œå®‰è£…å’Œä¸‹è½½
print("ğŸš€ å¼€å§‹å¹¶è¡Œå®‰è£…å’Œä¸‹è½½...")
thread1 = threading.Thread(target=install_dependencies)
thread2 = threading.Thread(target=download_model)
thread3 = threading.Thread(target=check_gpu)

thread1.start()
thread2.start()
thread3.start()

# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
thread1.join()
thread2.join()
thread3.join()

print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
```

**è¿è¡Œæ—¶é—´ï¼š** 10-15åˆ†é’Ÿ

### æ­¥éª¤4ï¼šåˆ›å»ºCell 2 - å¯åŠ¨vLLMæœåŠ¡

```python
# Cell 2: å¯åŠ¨vLLMæœåŠ¡
import subprocess
import threading
import time
import os

def start_vllm():
    try:
        print("ğŸš€ å¯åŠ¨vLLMæœåŠ¡...")
        subprocess.run([
            'python', '-m', 'vllm.entrypoints.openai.api_server',
            '--model', 'meta-llama/Llama-2-7b-chat-hf',
            '--port', '8000',
            '--host', '0.0.0.0',
            '--max-model-len', '2048'
        ])
    except Exception as e:
        print(f"âŒ Llama-2å¯åŠ¨å¤±è´¥: {e}")
        print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹...")
        try:
            subprocess.run([
                'python', '-m', 'vllm.entrypoints.openai.api_server',
                '--model', 'microsoft/DialoGPT-medium',
                '--port', '8000',
                '--host', '0.0.0.0'
            ])
        except Exception as e2:
            print(f"âŒ å¤‡ç”¨æ¨¡å‹ä¹Ÿå¯åŠ¨å¤±è´¥: {e2}")

print("ğŸ”„ å¯åŠ¨vLLMæœåŠ¡...")
vllm_thread = threading.Thread(target=start_vllm)
vllm_thread.daemon = True
vllm_thread.start()

# ç­‰å¾…æœåŠ¡å¯åŠ¨
print("â³ ç­‰å¾…æœåŠ¡å¯åŠ¨...")
for i in range(30):
    print(f"ç­‰å¾…ä¸­... {i+1}/30")
    time.sleep(2)
    
print("âœ… vLLMæœåŠ¡åº”è¯¥å·²ç»å¯åŠ¨ï¼")
```

**è¿è¡Œæ—¶é—´ï¼š** 5-10åˆ†é’Ÿ

### æ­¥éª¤5ï¼šåˆ›å»ºCell 3 - æš´éœ²ç«¯å£

```python
# Cell 3: ä½¿ç”¨ngrokæš´éœ²ç«¯å£
from pyngrok import ngrok
import time

print("ğŸŒ æ­£åœ¨æš´éœ²ç«¯å£...")
try:
    # æš´éœ²8000ç«¯å£
    public_url = ngrok.connect(8000)
    print("âœ… ç«¯å£æš´éœ²æˆåŠŸï¼")
    print("=" * 60)
    print(f"ğŸ”— Public URL: {public_url}")
    print(f"ğŸ”— API URL: {public_url}/v1")
    print("=" * 60)
    print("ğŸ“‹ é‡è¦ï¼šè¯·å¤åˆ¶ä¸Šé¢çš„URLåˆ°ä½ çš„æ¸¸æˆé…ç½®ä¸­ï¼")
    print("=" * 60)
    
    # ä¿å­˜URLåˆ°å˜é‡ï¼Œä¾›ä¸‹ä¸€ä¸ªCellä½¿ç”¨
    globals()['ngrok_url'] = str(public_url)
    
except Exception as e:
    print(f"âŒ æš´éœ²ç«¯å£å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥ngrokæ˜¯å¦æ­£ç¡®å®‰è£…")
```

### æ­¥éª¤6ï¼šåˆ›å»ºCell 4 - æµ‹è¯•æœåŠ¡

```python
# Cell 4: æµ‹è¯•vLLMæœåŠ¡
import requests
import json
import time

# ç­‰å¾…ä¸€ä¸‹è®©æœåŠ¡å®Œå…¨å¯åŠ¨
print("â³ ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨...")
time.sleep(10)

# æµ‹è¯•API
test_url = f"{ngrok_url}/v1/chat/completions"
test_data = {
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [
        {"role": "user", "content": "Hello, how are you?"}
    ],
    "max_tokens": 50
}

print("ğŸ§ª æµ‹è¯•vLLMæœåŠ¡...")
try:
    response = requests.post(test_url, json=test_data, timeout=30)
    if response.status_code == 200:
        print("âœ… vLLMæœåŠ¡è¿è¡Œæ­£å¸¸ï¼")
        result = response.json()
        ai_reply = result['choices'][0]['message']['content']
        print(f"ğŸ¤– AIå›å¤: {ai_reply}")
        print("ğŸ‰ æœåŠ¡æµ‹è¯•æˆåŠŸï¼å¯ä»¥å¼€å§‹ä½¿ç”¨AIåˆ†æåŠŸèƒ½äº†ï¼")
    else:
        print(f"âŒ æœåŠ¡æµ‹è¯•å¤±è´¥: {response.status_code}")
        print(f"å“åº”: {response.text}")
except Exception as e:
    print(f"âŒ è¿æ¥å¤±è´¥: {str(e)}")
    print("è¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸å¯åŠ¨")
```

## ğŸ”§ é…ç½®æ¸¸æˆ

### ä¿®æ”¹script.jsæ–‡ä»¶

åœ¨ `script.js` ä¸­æ‰¾åˆ° `AI_CONFIG` å¯¹è±¡ï¼Œä¿®æ”¹ä¸ºï¼š

```javascript
const AI_CONFIG = {
    // é€‰æ‹©AIåç«¯: 'vllm', 'openai', 'claude', 'gemini', 'mock'
    backend: 'vllm', // ä½¿ç”¨Colabçš„vLLMæœåŠ¡
    
    // vLLMé…ç½®
    vllm: {
        baseUrl: 'https://YOUR_NGROK_URL/v1', // æ›¿æ¢ä¸ºä½ çš„ngrok URL
        model: 'meta-llama/Llama-2-7b-chat-hf', // æˆ–ä½ ä½¿ç”¨çš„æ¨¡å‹å
        apiKey: 'your-vllm-api-key' // é€šå¸¸ä¸éœ€è¦
    }
};
```

**é‡è¦ï¼š** å°† `YOUR_NGROK_URL` æ›¿æ¢ä¸ºCell 3ä¸­æ˜¾ç¤ºçš„ngrok URL

### ç¤ºä¾‹é…ç½®

```javascript
// ç¤ºä¾‹ï¼šå¦‚æœä½ çš„ngrok URLæ˜¯ https://abc123.ngrok.io
vllm: {
    baseUrl: 'https://abc123.ngrok.io/v1',
    model: 'meta-llama/Llama-2-7b-chat-hf',
    apiKey: 'your-vllm-api-key'
}
```

## ğŸ® æµ‹è¯•æ¸¸æˆ

1. æ‰“å¼€ä½ çš„æ·ç¡¬å¸æ¸¸æˆ
2. æ³¨å†Œ/ç™»å½•è´¦å·
3. ç©å‡ å±€æ¸¸æˆï¼ˆè‡³å°‘3æ¬¡ï¼‰
4. ç‚¹å‡»AIåˆ†ææŒ‰é’®
5. æŸ¥çœ‹æ˜¯å¦è¿æ¥æˆåŠŸ

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### Colabé™åˆ¶
- **ä¼šè¯æ—¶é—´**ï¼šæœ€é•¿12å°æ—¶
- **GPUæ—¶é—´**ï¼šæ¯å¤©æœ‰é™åˆ¶
- **ç½‘ç»œ**ï¼šå¯èƒ½ä¸ç¨³å®š

### è§£å†³æ–¹æ¡ˆ
- **ä¿å­˜notebook**ï¼šå®šæœŸä¿å­˜ä»£ç 
- **ç›‘æ§æ—¶é—´**ï¼šé¿å…è¶…æ—¶
- **å¤‡ç”¨æ–¹æ¡ˆ**ï¼šå‡†å¤‡å…¶ä»–AIæœåŠ¡

### æˆæœ¬
- **Colab Pro**ï¼š$10/æœˆ
- **GPUæ—¶é—´**ï¼šæ¯å¤©æœ€å¤š12å°æ—¶
- **é€‚åˆ**ï¼šå­¦ä¹ å’Œå®éªŒ

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å‹ä¸‹è½½å¤±è´¥
```python
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')
```

#### 2. æœåŠ¡å¯åŠ¨å¤±è´¥
```python
# æ£€æŸ¥GPUå†…å­˜
!nvidia-smi

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
subprocess.run([
    'python', '-m', 'vllm.entrypoints.openai.api_server',
    '--model', 'microsoft/DialoGPT-medium',
    '--port', '8000',
    '--host', '0.0.0.0'
])
```

#### 3. è¿æ¥å¤±è´¥
- æ£€æŸ¥ngrok URLæ˜¯å¦æ­£ç¡®
- ç¡®è®¤Colab notebookè¿˜åœ¨è¿è¡Œ
- æŸ¥çœ‹æ§åˆ¶å°é”™è¯¯ä¿¡æ¯

#### 4. æ¸¸æˆæ— æ³•è¿æ¥
- ç¡®è®¤script.jsä¸­çš„URLæ­£ç¡®
- æ£€æŸ¥CORSè®¾ç½®
- æŸ¥çœ‹æµè§ˆå™¨æ§åˆ¶å°é”™è¯¯

### è°ƒè¯•å‘½ä»¤

```python
# æ£€æŸ¥GPUçŠ¶æ€
!nvidia-smi

# æ£€æŸ¥ç«¯å£å ç”¨
!netstat -tlnp | grep 8000

# æ£€æŸ¥è¿›ç¨‹
!ps aux | grep vllm

# æµ‹è¯•æœ¬åœ°è¿æ¥
!curl http://localhost:8000/v1/models
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### æ¨¡å‹é€‰æ‹©
- **Llama-2-7b**ï¼šåŠŸèƒ½å¼ºå¤§ï¼Œéœ€è¦æ›´å¤šèµ„æº
- **DialoGPT-medium**ï¼šè½»é‡çº§ï¼Œé€‚åˆæµ‹è¯•

### å‚æ•°è°ƒæ•´
```python
# å‡å°‘å†…å­˜ä½¿ç”¨
'--max-model-len', '1024'  # å‡å°‘åˆ°1024

# ä½¿ç”¨æ›´å°çš„batch size
'--max-num-batched-tokens', '2048'
```

## ğŸ”„ é‡å¯æœåŠ¡

å¦‚æœæœåŠ¡å‡ºç°é—®é¢˜ï¼Œå¯ä»¥é‡æ–°è¿è¡ŒCell 2-4ï¼š

1. åœæ­¢å½“å‰æœåŠ¡ï¼ˆCtrl+Cï¼‰
2. é‡æ–°è¿è¡ŒCell 2ï¼ˆå¯åŠ¨æœåŠ¡ï¼‰
3. é‡æ–°è¿è¡ŒCell 3ï¼ˆæš´éœ²ç«¯å£ï¼‰
4. é‡æ–°è¿è¡ŒCell 4ï¼ˆæµ‹è¯•æœåŠ¡ï¼‰

## ğŸ“ˆ ç›‘æ§å’Œç»´æŠ¤

### ç›‘æ§æœåŠ¡çŠ¶æ€
```python
# æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
import requests
try:
    response = requests.get(f"{ngrok_url}/v1/models")
    if response.status_code == 200:
        print("âœ… æœåŠ¡è¿è¡Œæ­£å¸¸")
    else:
        print("âŒ æœåŠ¡å¼‚å¸¸")
except:
    print("âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡")
```

### ä¿å­˜é‡è¦ä¿¡æ¯
```python
# ä¿å­˜é…ç½®ä¿¡æ¯
config_info = {
    'ngrok_url': ngrok_url,
    'model': 'meta-llama/Llama-2-7b-chat-hf',
    'timestamp': time.time()
}

print("ğŸ“‹ é…ç½®ä¿¡æ¯:")
for key, value in config_info.items():
    print(f"{key}: {value}")
```

## ğŸ¯ æ€»ç»“

é€šè¿‡è¿™ä¸ªæŒ‡å—ï¼Œä½ å¯ä»¥ï¼š

1. âœ… åœ¨Google Colabä¸Šéƒ¨ç½²vLLM
2. âœ… å°†AIæœåŠ¡é›†æˆåˆ°ä½ çš„æ¸¸æˆä¸­
3. âœ… äº«å—çœŸå®çš„AIåˆ†æåŠŸèƒ½
4. âœ… èŠ‚çœæœ¬åœ°è®¡ç®—èµ„æº

**æ€»æˆæœ¬ï¼š** $10/æœˆï¼ˆColab Proï¼‰
**éƒ¨ç½²æ—¶é—´ï¼š** 20-30åˆ†é’Ÿ
**ç»´æŠ¤éš¾åº¦ï¼š** ä¸­ç­‰

ç°åœ¨ä½ çš„æ·ç¡¬å¸æ¸¸æˆå·²ç»æ˜¯ä¸€ä¸ªå®Œæ•´çš„AIé©±åŠ¨çš„æ™ºèƒ½æ¸¸æˆäº†ï¼ğŸ®ğŸ¤–
